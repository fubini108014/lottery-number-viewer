import requests
from bs4 import BeautifulSoup
import json
import os
from datetime import datetime

# ç›®éŒ„èˆ‡æª”æ¡ˆè¨­å®š
DATA_DIR = "public/data"
INDEX_FILE = "public/files.json"
TARGET_URL = "https://www.pilio.idv.tw/lto539/list.asp"
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
}

if not os.path.exists(DATA_DIR):
    os.makedirs(DATA_DIR)

def fetch_web_data():
    print(f"ğŸ“¡ æ­£åœ¨æŠ“å–ç¶²é è³‡æ–™...")
    try:
        res = requests.get(TARGET_URL, headers=HEADERS)
        res.encoding = 'big5'
        soup = BeautifulSoup(res.text, 'html.parser')
        
        extracted = []
        # å°‹æ‰¾æ‰€æœ‰è³‡æ–™åˆ— (åŒ…å« date-cell çš„ tr)
        rows = soup.find_all('tr')
        
        for row in rows:
            date_td = row.find('td', class_='date-cell')
            num_td = row.find('td', class_='number-cell')
            
            if date_td and num_td:
                # 1. è™•ç†æ—¥æœŸ (æ ¼å¼å¦‚: 02/19\n26(å››))
                # æˆ‘å€‘éœ€è¦æŠŠ 02/19 å’Œ 26 æ‹¼æ¹Šæˆ 2026/02/19
                raw_date_text = date_td.get_text("|", strip=True) # ä½¿ç”¨ | åˆ†éš” br æ¨™ç±¤
                parts = raw_date_text.split("|")
                if len(parts) >= 2:
                    month_day = parts[0] # "02/19"
                    year_short = parts[1][:2] # "26"
                    full_date = f"20{year_short}/{month_day}" # "2026/02/19"
                else:
                    continue

                # 2. è™•ç†è™Ÿç¢¼ (æ ¼å¼å¦‚: 08, 15, 19, 25, 27)
                raw_nums = num_td.get_text(strip=True).replace("\xa0", "") # å»é™¤ &nbsp;
                try:
                    num_list = [int(n.strip()) for n in raw_nums.split(',')]
                    if len(num_list) == 5:
                        print(f"âœ¨ æ‰¾åˆ°è³‡æ–™: {full_date} -> {num_list}")
                        extracted.append({"date": full_date, "numbers": num_list})
                except:
                    continue
                    
        return extracted
    except Exception as e:
        print(f"âŒ æŠ“å–å¤±æ•—: {e}")
        return []

def update_json_data(new_records):
    if not new_records:
        print("âš ï¸ è­¦å‘Šï¼šæœªæŠ“å–åˆ°ä»»ä½•è³‡æ–™ï¼Œè«‹æª¢æŸ¥ç¶²é æ˜¯å¦æ”¹ç‰ˆã€‚")
        return

    # è®€å–ç´¢å¼•æª”
    index_list = []
    if os.path.exists(INDEX_FILE):
        with open(INDEX_FILE, 'r', encoding='utf-8') as f:
            index_list = json.load(f)

    # ä¾æ—¥æœŸç”±èˆŠåˆ°æ–°è™•ç†ï¼Œç¢ºä¿è¿½åŠ é †åºæ­£ç¢º
    new_records.sort(key=lambda x: x['date'])

    for record in new_records:
        year = record['date'].split('/')[0]
        data_file_path = os.path.join(DATA_DIR, f"lottery_{year}.json")
        
        year_data = []
        if os.path.exists(data_file_path):
            with open(data_file_path, 'r', encoding='utf-8') as f:
                year_data = json.load(f)
        
        # æª¢æŸ¥é‡è¤‡
        if not any(item['date'] == record['date'] for item in year_data):
            year_data.append(record)
            # å­˜æª”æ™‚ç”±æ–°åˆ°èˆŠæ’ (æœ€æ–°åœ¨ä¸Šé¢)
            year_data.sort(key=lambda x: x['date'], reverse=True)
            with open(data_file_path, 'w', encoding='utf-8') as f:
                json.dump(year_data, f, indent=2, ensure_ascii=False)
            print(f"âœ… å·²å­˜æª”: {record['date']}")
            
            # æ›´æ–°ç´¢å¼•
            rel_path = f"lottery_{year}.json"
            if not any(idx['year'] == year for idx in index_list):
                index_list.append({"name": rel_path, "year": year})
                index_list.sort(key=lambda x: x['year'], reverse=True)
                with open(INDEX_FILE, 'w', encoding='utf-8') as f:
                    json.dump(index_list, f, indent=2, ensure_ascii=False)
        else:
            print(f"â­ï¸ {record['date']} å·²å­˜åœ¨ï¼Œè·³éã€‚")

if __name__ == "__main__":
    records = fetch_web_data()
    update_json_data(records)
    print("ğŸ åŸ·è¡Œå®Œç•¢")